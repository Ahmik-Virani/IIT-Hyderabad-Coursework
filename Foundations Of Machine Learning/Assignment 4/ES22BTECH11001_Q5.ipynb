{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the sigmoid function for later use\n",
    "#sigmoid(x) = 1 / (1 + e^(-x))\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    # This is the initialization function\n",
    "    def __init__(self, learning_rate=0.1, no_iterations=1000, run_till_convergence = False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_iterations = no_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        # The below two are required when we need to run till convergence\n",
    "        self.prev_weights = None\n",
    "        self.prev_bias = None\n",
    "        self.run_till_convergence = run_till_convergence\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        no_samples, no_features = X.shape\n",
    "        self.weights = [1.5, 0.5]                   # Initialize weights as given in the question\n",
    "        self.bias = -1                              # Initialize bias as given in the question\n",
    "\n",
    "        # We need to run till convergence\n",
    "        if self.run_till_convergence == True:\n",
    "            no_of_iterations = 0\n",
    "            while True:\n",
    "                linear_predictions = np.dot(X, self.weights) + self.bias        # y = w.X + b\n",
    "                predictions = sigmoid(linear_predictions)                       # Predict using sigmoid function\n",
    "\n",
    "                # This is to find the gradients for weights and bias using cross entropy error\n",
    "                dw = (1/no_samples) * np.dot(X.T, (predictions - y))\n",
    "                db = (1/no_samples) * np.sum(predictions - y)\n",
    "\n",
    "                # Perform update in gradient descent\n",
    "                self.prev_weights = self.weights\n",
    "                self.prev_bias = self.bias\n",
    "                self.weights = self.weights - self.learning_rate * dw\n",
    "                self.bias = self.bias - self.learning_rate * db\n",
    "\n",
    "                weight_diff = np.linalg.norm(self.weights - self.prev_weights)  # L2 norm of weight differences\n",
    "                bias_diff = abs(self.bias - self.prev_bias)\n",
    "\n",
    "                no_of_iterations += 1\n",
    "\n",
    "                if weight_diff < 1e-5 and bias_diff < 1e-5:\n",
    "                    break\n",
    "            \n",
    "            return self.weights, self.bias, no_of_iterations\n",
    "\n",
    "        for _ in range(self.no_iterations):\n",
    "            linear_predictions = np.dot(X, self.weights) + self.bias        # y = w.X + b\n",
    "            predictions = sigmoid(linear_predictions)                       # Predict using sigmoid function\n",
    "\n",
    "            # This is to find the gradients for weights and bias using cross entropy error\n",
    "            dw = (1/no_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1/no_samples) * np.sum(predictions - y)\n",
    "\n",
    "            # Perform update in gradient descent\n",
    "            self.weights = self.weights - self.learning_rate * dw\n",
    "            self.bias = self.bias - self.learning_rate * db\n",
    "        \n",
    "        return self.weights, self.bias, self.no_iterations\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "            linear_predictions = np.dot(X, self.weights) + self.bias        # y = w.X + b\n",
    "            y_predictions = sigmoid(linear_predictions)\n",
    "\n",
    "            # Calculations of the class labels after the probability is calculated\n",
    "            class_predictions = [0 if y<=0.5 else 1 for y in y_predictions]\n",
    "            return class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "weights:  [1.50535086 0.50196867]\n",
      "bias:  -1.0031662597725644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.array([[0.346, 0.780], [0.303, 0.439], [0.358, 0.729], [0.602, 0.863], [0.790, 0.753], [0.611, 0.965]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Convert X_test and y_test to NumPy arrays\n",
    "X_test = np.array([[0.959, 0.382], [0.750, 0.306], [0.395, 0.760], [0.823, 0.764], [0.761, 0.874], [0.844, 0.435]])\n",
    "y_test = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "clf = LogisticRegression(learning_rate=0.1, no_iterations=1)\n",
    "weights, bias, no_iterations = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print(acc)\n",
    "print('weights: ', weights)\n",
    "print('bias: ', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "weights:  [45.66010049 10.02325155]\n",
      "bias:  -30.067300721813744\n",
      "no of iterations:  736387\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(learning_rate=0.1, run_till_convergence=True)\n",
    "weights, bias, no_iterations = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print(acc)\n",
    "print('weights: ', weights)\n",
    "print('bias: ', bias)\n",
    "print('no of iterations: ', no_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.6 , Recall:  1.0 , Accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "def precision(y_pred, y_test):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for index in range(6):\n",
    "        if(y_pred[index] == y_test[index] == 1):\n",
    "            TP+=1\n",
    "        elif(y_pred[index]==1 and y_test[index]==0):\n",
    "            FP+=1\n",
    "\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def recall(y_pred, y_test):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    for index in range(6):\n",
    "        if(y_pred[index] == y_test[index] == 1):\n",
    "            TP+=1\n",
    "        elif(y_pred[index]==0 and y_test[index]==1):\n",
    "            FN+=1\n",
    "    \n",
    "    return TP / (TP + FN)\n",
    "\n",
    "print('Precision: ', precision(y_pred, y_test), ', Recall: ', recall(y_pred, y_test), ', Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
