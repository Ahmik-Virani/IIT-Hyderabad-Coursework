{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations Of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name : Ahmik Virani <br>\n",
    "Roll Number : ES22BTECH11001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "red_wine = pd.read_csv('winequality-red.csv', sep=';')\n",
    "white_wine = pd.read_csv('winequality-white.csv', sep=';')\n",
    "\n",
    "# Combine datasets\n",
    "wine_data = pd.concat([red_wine, white_wine], ignore_index=True)\n",
    "\n",
    "# Create binary classification: 1 if quality >= 7, else 0\n",
    "wine_data['quality'] = (wine_data['quality'] >= 7).astype(int)\n",
    "\n",
    "# Split features and labels\n",
    "X = wine_data.drop('quality', axis=1).values  # Features\n",
    "y = wine_data['quality'].values  # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Node of the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left_child=None, right_child=None, value=None):\n",
    "        self.feature = feature          # Index of the feature for splitting the node\n",
    "        self.threshold = threshold      # Value of feature used for splitting the node\n",
    "        self.left = left_child          # Left child: these have vales <= threshold\n",
    "        self.right = right_child        # Right child: these have vales > threshold\n",
    "        self.value = value              # This is the predicted value of this wis a leaf node\n",
    "    \n",
    "    # This is a method to check if this is a leaf node\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) : Implement your own version of the decision tree using binary univariate split, entropy and information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_sample_split=2, n_features=None, random_state=None):\n",
    "        # Initialize the decision tree parameters\n",
    "        self.min_samples_split = min_sample_split   # Define the minimum number of samples to split the node\n",
    "        self.n_features = n_features                # Define the number of features to consider for splitting\n",
    "        self.root = None                            # Define the root of the decision tree\n",
    "        self.random_state = random_state            # To ensure reproducability, keep the random_state fixed\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Determine the number of features to consider for each split\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)       # Grow the decision tree by finding the best split\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape        # Check the number of sample and features we are working on\n",
    "        n_labels = len(np.unique(y))        # Find the number of unique labels\n",
    "        \n",
    "        # This is the stopping condition, note that here we allow the tree to grow while splitting is possible, we do not do pruning here\n",
    "        if (n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)           # Randomly select features for splitting\n",
    "\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)                   # Find the best feature and threshold for splitting\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)            # Split the dataset based on best index and threshold\n",
    "\n",
    "        # Recusrively grow the tree\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        # Iterate over each feature\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            # Calculate information gain for each threshold\n",
    "            for thr in thresholds:\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                # Update the information gain if a better split is found\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "        \n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    # A function to calculate the entropy\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    # A function to calculate the information gain\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)                           # Compute the entropy of the parent node\n",
    "\n",
    "        # Split the data into left and right indexes based on the threshold\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # Find the weighted entropy of the child\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # Return the information gain\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        # Split the data based in threshold into left and right child indexes\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    # Function to extract the most occuring label\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    # Function to predict label for input samples\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    # Function to traverse the node to make predictions\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "    \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8554\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the decision tree classifier\n",
    "tree = DecisionTree(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = tree.predict(X_test)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the above observation:\n",
    "1. The accuracy of the model using train_test_split is 85.54%\n",
    "2. This accuracy is quite good considering the fact that question allowed accuracy above 78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): 10 Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle dataset\n",
    "sample_size = wine_data.shape[0]\n",
    "permuted_arr = np.random.permutation(sample_size)\n",
    "wine_data = wine_data.iloc[permuted_arr, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the 10 fold cross validation accuracy is :  0.8462336521477696\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with fixed seed\n",
    "K = 10\n",
    "section_size = wine_data.shape[0] // K\n",
    "\n",
    "tree = DecisionTree(random_state=42)  # Fixed random state\n",
    "acc = []\n",
    "\n",
    "# Going to all the 10 folds\n",
    "for i in range(K):\n",
    "    start_ind = i * section_size\n",
    "    end_ind = wine_data.shape[0] if i == K - 1 else start_ind + section_size\n",
    "    \n",
    "    X_train = pd.concat([wine_data.iloc[0:start_ind, :-1], wine_data.iloc[end_ind:, :-1]]).values\n",
    "    y_train = pd.concat([wine_data['quality'].iloc[0:start_ind], wine_data['quality'].iloc[end_ind:]]).values\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    unseen_data = wine_data.iloc[start_ind:end_ind, :-1].values\n",
    "    y_pred = tree.predict(unseen_data)\n",
    "    y_actual = wine_data['quality'].iloc[start_ind:end_ind].values\n",
    "    \n",
    "    acc.append(accuracy_score(y_actual, y_pred))\n",
    "    \n",
    "acc_np = np.array(acc)\n",
    "print(\"The mean of the 10 fold cross validation accuracy is : \", np.mean(acc_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While 10 fold cross validation, the best accuracy is:  0.8582434514637904\n"
     ]
    }
   ],
   "source": [
    "i = np.argmax(acc_np)\n",
    "print(\"While 10 fold cross validation, the best accuracy is: \", acc_np[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the above observations:\n",
    "1. The average accuracy is 84.62%\n",
    "2. The best result is 85.82%\n",
    "3. As it should, the best result is more than the given value in part(a) above, which was: 85.54%\n",
    "4. This can be used to validate that different training sets can form different tree, this happens because decision trees are sensitive to the data they are trained on\n",
    "5. We can see that the percentage difference between best result and average result is slightly above 1%, which means more often than not, my model is quite consistent and gives similar accurate results. This indicates a stable model with minimal variance, which is a desirable characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same decision tree as before, except that  I have changed the entropy with gini index\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_sample_split=2, n_features=None, random_state=None):\n",
    "        self.min_samples_split = min_sample_split\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        self.random_state = random_state\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        if (n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                gain = self._information_gain_with_info_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "        \n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _gini_index(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return 1 - np.sum([p ** 2 for p in ps if p > 0])\n",
    "\n",
    "    def _information_gain_with_info_gain(self, y, X_column, threshold):\n",
    "        parent_gini_index = self._gini_index(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        gini_index_l, gini_index_r = self._gini_index(y[left_idxs]), self._gini_index(y[right_idxs])\n",
    "        child_gini_index = (n_l / n) * gini_index_l + (n_r / n) * gini_index_r\n",
    "\n",
    "        information_gain = parent_gini_index - child_gini_index\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "    \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Shuffle dataset\n",
    "sample_size = wine_data.shape[0]\n",
    "permuted_arr = np.random.permutation(sample_size)\n",
    "wine_data = wine_data.iloc[permuted_arr, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the accuracy using Gini Index instead of Entropy is :  0.8456354053139915\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with fixed seed\n",
    "K = 10\n",
    "section_size = wine_data.shape[0] // K\n",
    "\n",
    "obj = DecisionTree(random_state=42)  # Fixed random state\n",
    "acc = []\n",
    "\n",
    "for i in range(K):\n",
    "    start_ind = i * section_size\n",
    "    end_ind = wine_data.shape[0] if i == K - 1 else start_ind + section_size\n",
    "        \n",
    "    X_train = pd.concat([wine_data.iloc[0:start_ind, :-1], wine_data.iloc[end_ind:, :-1]]).values\n",
    "    y_train = pd.concat([wine_data['quality'].iloc[0:start_ind], wine_data['quality'].iloc[end_ind:]]).values\n",
    "    obj.fit(X_train, y_train)\n",
    "        \n",
    "    unseen_data = wine_data.iloc[start_ind:end_ind, :-1].values\n",
    "    y_pred = obj.predict(unseen_data)\n",
    "    y_actual = wine_data['quality'].iloc[start_ind:end_ind].values\n",
    "    \n",
    "    acc.append(accuracy_score(y_actual, y_pred))\n",
    "    \n",
    "acc_np = np.array(acc)\n",
    "print(\"Calculating the accuracy using Gini Index instead of Entropy is : \", np.mean(acc_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8690292758089369\n"
     ]
    }
   ],
   "source": [
    "i = np.argmax(acc_np)\n",
    "print(acc_np[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the above observations:\n",
    "1. The average accuracy of the cross validation is : 84.56%\n",
    "2. The best accuracy is : 86.90%\n",
    "3. Clearly, the average is close to the results of entopy, underperforming by just 0.06%, whereas the best case is much better.\n",
    "4. We can see the tradeoff between varying entropy v/s gini index, where in entropy the results are quite close with low variance, whereas the better result overall is using gini index\n",
    "\n",
    "From these results I come to the following conclusion:  Entropy and Gini Index measure impurity slightly differently, leading to minor variations in splits and performance. Gini can sometimes find more aggressive splits, which may explain the higher best-case performance, while entropy often leads to more balanced splits, resulting in lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same decision tree as before except I have introduced pruning\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_sample_split=2, max_depth=100, n_features=None, random_state=None):\n",
    "        self.min_samples_split = min_sample_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        self.random_state = random_state\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "        \n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return 1 - np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "    \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Shuffle dataset\n",
    "sample_size = wine_data.shape[0]\n",
    "permuted_arr = np.random.permutation(sample_size)\n",
    "wine_data = wine_data.iloc[permuted_arr, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8517674940809499\n",
      "[0.80344785 0.80175293 0.80851451 0.82282451 0.83022708 0.83051717\n",
      " 0.8286731  0.82991563 0.83191378 0.82697818 0.83437911 0.83867535\n",
      " 0.84176195 0.84237664 0.84576318 0.84360602 0.84883498 0.84728922\n",
      " 0.84807114 0.84591726 0.84868254 0.85145933 0.85176749 0.84929723\n",
      " 0.85145275 0.84992014 0.84945789 0.85022666 0.85022666 0.85022666]\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with fixed seed\n",
    "K = 10\n",
    "section_size = wine_data.shape[0] // K\n",
    "\n",
    "values = []\n",
    "for d in range(1, 31, 1):\n",
    "    obj = DecisionTree(max_depth=d, random_state=42)  # Fixed random state\n",
    "    acc = []\n",
    "    for i in range(K):\n",
    "        start_ind = i * section_size\n",
    "        end_ind = wine_data.shape[0] if i == K - 1 else start_ind + section_size\n",
    "        \n",
    "        x_train = pd.concat([wine_data.iloc[0:start_ind, :-1], wine_data.iloc[end_ind:, :-1]]).values\n",
    "        y_train = pd.concat([wine_data['quality'].iloc[0:start_ind], wine_data['quality'].iloc[end_ind:]]).values\n",
    "        obj.fit(x_train, y_train)\n",
    "        \n",
    "        unseen_data = wine_data.iloc[start_ind:end_ind, :-1].values\n",
    "        y_pred = obj.predict(unseen_data)\n",
    "        y_actual = wine_data['quality'].iloc[start_ind:end_ind].values\n",
    "        \n",
    "        acc.append(accuracy_score(y_actual, y_pred))\n",
    "    \n",
    "    acc_np = np.array(acc)\n",
    "    values.append(np.mean(acc_np))\n",
    "\n",
    "values = np.array(values)\n",
    "i = np.argmax(values)\n",
    "print(values[i])\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The depth of the decision tree giving the best result it :  23\n",
      "The best accuracy is:  0.8517674940809499\n"
     ]
    }
   ],
   "source": [
    "print(\"The depth of the decision tree giving the best result it : \", i + 1)\n",
    "print(\"The best accuracy is: \", values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations from the above results are as follows:\n",
    "1. The depth at which the best result is observed is : 23, whereas the max depth is 26.\n",
    "- **Why am I claiming the depth is 26:** Note that we can see that after depth 26, the value becomes constant, which means that it is always going to the same level\n",
    "2. The accuracy at this depth : 85.17%\n",
    "3. This is better than the average of both gini index and entropy accuracies.\n",
    "4. This shows that the model, to some extent overfits\n",
    "5. Also, by removing some nodes at the end, we are limiting the over fitting and generalizing to some extent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
